{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b8f5e9-53aa-4924-9bdf-a1a0c4615a62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Word!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello Word!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4aa7bb5-3cb2-4c9a-9eb0-e95c6d4c3c43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##  Configuration of the current Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0d86d8-6528-429c-9a91-ac261d22cd07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.files.useFetchCache = false\n",
      "spark.databricks.preemption.enabled = true\n",
      "spark.databricks.clusterUsageTags.clusterFirstOnDemand = 1\n",
      "spark.databricks.clusterUsageTags.dataPlaneRegion = eu-west-3\n",
      "spark.sql.hive.metastore.jars = /databricks/databricks-hive/*\n",
      "spark.driver.tempDirectory = /local_disk0/tmp\n",
      "spark.sql.warehouse.dir = dbfs:/user/hive/warehouse\n",
      "spark.databricks.managedCatalog.clientClassName = com.databricks.managedcatalog.ManagedCatalogClientImpl\n",
      "spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName = com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider\n",
      "spark.master = spark://10.233.171.33:7077\n",
      "spark.hadoop.fs.fcfs-s3.impl.disable.cache = true\n",
      "spark.sql.streaming.checkpointFileManagerClass = com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager\n",
      "spark.databricks.clusterUsageTags.clusterAvailability = SPOT_WITH_FALLBACK\n",
      "spark.databricks.service.dbutils.repl.backend = com.databricks.dbconnect.ReplDBUtils\n",
      "spark.hadoop.databricks.s3.verifyBucketExists.enabled = false\n",
      "spark.r.sql.derby.temp.dir = /tmp/RtmpfXYxC7\n",
      "spark.streaming.driver.writeAheadLog.allowBatching = true\n",
      "spark.databricks.clusterSource = UI\n",
      "spark.hadoop.hive.server2.transport.mode = http\n",
      "spark.hadoop.fs.cpfs-adl.impl.disable.cache = true\n",
      "spark.databricks.clusterUsageTags.hailEnabled = false\n",
      "spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled = false\n",
      "spark.databricks.clusterUsageTags.containerType = LXC\n",
      "spark.hadoop.fs.s3a.assumed.role.credentials.provider = shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksInstanceProfileCredentialsProvider\n",
      "spark.eventLog.enabled = false\n",
      "spark.hadoop.fs.stage.impl.disable.cache = true\n",
      "spark.hadoop.hive.hmshandler.retry.interval = 2000\n",
      "spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb = 100\n",
      "spark.executor.tempDirectory = /local_disk0/tmp\n",
      "spark.hadoop.fs.azure.authorization.caching.enable = false\n",
      "spark.hadoop.fs.fcfs-abfss.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
      "spark.databricks.clusterUsageTags.driverContainerId = 8022b4994dce43fe80b78ece1ef431a5\n",
      "spark.hadoop.mapred.output.committer.class = com.databricks.backend.daemon.data.client.DirectOutputCommitter\n",
      "spark.databricks.clusterUsageTags.clusterTargetWorkers = 2\n",
      "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version = 2\n",
      "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3 = 0\n",
      "spark.sql.allowMultipleContexts = false\n",
      "spark.databricks.eventLog.enabled = true\n",
      "spark.hadoop.hive.server2.thrift.http.port = 10000\n",
      "spark.home = /databricks/spark\n",
      "spark.hadoop.hive.server2.idle.operation.timeout = 7200000\n",
      "spark.task.reaper.enabled = true\n",
      "spark.storage.memoryFraction = 0.5\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dio.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1\n",
      "spark.databricks.sql.configMapperClass = com.databricks.dbsql.config.SqlConfigMapperBridge\n",
      "spark.driver.maxResultSize = 4g\n",
      "spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline = false\n",
      "spark.hadoop.fs.fcfs-s3.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
      "spark.databricks.delta.multiClusterWrites.enabled = true\n",
      "spark.worker.cleanup.enabled = false\n",
      "spark.sql.legacy.createHiveTableByDefault = false\n",
      "spark.databricks.driver.preferredMavenCentralMirrorUrl = https://maven-central.storage-download.googleapis.com/maven2/\n",
      "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File = 0\n",
      "spark.hadoop.fs.fcfs-s3a.impl.disable.cache = true\n",
      "spark.databricks.driverNodeTypeId = m6i.large\n",
      "spark.ui.port = 40001\n",
      "spark.databricks.clusterUsageTags.clusterName = Nacir Nacir's Cluster\n",
      "spark.databricks.clusterUsageTags.userId = 6774191665194252\n",
      "spark.hadoop.fs.s3a.attempts.maximum = 10\n",
      "spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign = false\n",
      "spark.databricks.clusterUsageTags.enableCredentialPassthrough = false\n",
      "spark.databricks.clusterUsageTags.effectiveSparkVersion = 13.3.x-photon-scala2.12\n",
      "spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType = ebs_volume_type: GENERAL_PURPOSE_SSD\n",
      "\n",
      "spark.databricks.clusterUsageTags.enableJdbcAutoStart = true\n",
      "spark.hadoop.fs.azure.user.agent.prefix = \n",
      "spark.hadoop.fs.s3n.impl.disable.cache = true\n",
      "spark.executor.memory = 629m\n",
      "spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough = false\n",
      "spark.hadoop.fs.fcfs-s3n.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
      "spark.hadoop.fs.abfs.impl = com.databricks.common.filesystem.LokiFileSystem\n",
      "spark.hadoop.fs.s3a.retry.throttle.interval = 500ms\n",
      "spark.hadoop.fs.wasb.impl.disable.cache = true\n",
      "spark.databricks.clusterUsageTags.clusterLogDestination = \n",
      "spark.databricks.wsfsPublicPreview = true\n",
      "spark.databricks.clusterUsageTags.clusterMinWorkers = 2\n",
      "spark.cleaner.referenceTracking.blocking = false\n",
      "spark.databricks.clusterUsageTags.clusterState = Pending\n",
      "spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes = false\n",
      "spark.databricks.tahoe.logStore.azure.class = com.databricks.tahoe.store.AzureLogStore\n",
      "spark.databricks.workspaceUrl = dbc-e8895296-d6a3.cloud.databricks.com\n",
      "spark.hadoop.fs.azure.skip.metrics = true\n",
      "spark.hadoop.hive.hmshandler.retry.attempts = 10\n",
      "spark.hadoop.fs.wasb.impl = com.databricks.common.filesystem.LokiFileSystem\n",
      "spark.scheduler.mode = FAIR\n",
      "spark.sql.sources.default = delta\n",
      "spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled = true\n",
      "spark.hadoop.fs.cpfs-s3n.impl = com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
      "spark.hadoop.fs.cpfs-adl.impl = com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
      "spark.hadoop.fs.fcfs-s3n.impl.disable.cache = true\n",
      "spark.hadoop.fs.cpfs-abfss.impl = com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
      "spark.databricks.passthrough.oauth.refresher.impl = com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient\n",
      "spark.sql.hive.metastore.sharedPrefixes = org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\n",
      "spark.databricks.io.directoryCommit.enableLogicalDelete = false\n",
      "spark.task.reaper.killTimeout = 60s\n",
      "spark.databricks.unityCatalog.enforce.permissions = false\n",
      "spark.hadoop.parquet.block.size.row.check.min = 10\n",
      "spark.hadoop.hive.server2.use.SSL = true\n",
      "spark.databricks.clusterUsageTags.driverInstancePrivateIp = 10.233.161.191\n",
      "spark.hadoop.spark.databricks.metrics.filesystem_metrics = true\n",
      "spark.hadoop.databricks.dbfs.client.version = v2\n",
      "spark.databricks.clusterUsageTags.clusterGeneration = 2\n",
      "spark.databricks.clusterUsageTags.driverInstanceId = i-0f37027a1dbd2178c\n",
      "spark.hadoop.hive.server2.keystore.path = /databricks/keys/jetty-ssl-driver-keystore.jks\n",
      "spark.hadoop.fs.gs.impl = com.databricks.common.filesystem.LokiFileSystem\n",
      "spark.databricks.credential.redactor = com.databricks.logging.secrets.CredentialRedactorProxyImpl\n",
      "spark.databricks.clusterUsageTags.region = eu-west-3\n",
      "spark.databricks.clusterUsageTags.clusterPinned = false\n",
      "spark.databricks.acl.provider = com.databricks.sql.acl.ReflectionBackedAclProvider\n",
      "spark.databricks.wsfs.workspacePrivatePreview = true\n",
      "spark.databricks.mlflow.autologging.enabled = true\n",
      "spark.extraListeners = com.databricks.backend.daemon.driver.DBCEventLoggingListener\n",
      "spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled = false\n",
      "spark.sql.parquet.cacheMetadata = true\n",
      "spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2 = 0\n",
      "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss = 0\n",
      "spark.hadoop.parquet.abfs.readahead.optimization.enabled = true\n",
      "spark.hadoop.fs.cpfs-abfss.impl.disable.cache = true\n",
      "spark.databricks.clusterUsageTags.userProvidedSparkVersion = 13.3.x-scala2.12\n",
      "spark.hadoop.fs.abfss.impl = com.databricks.common.filesystem.LokiFileSystem\n",
      "spark.databricks.clusterUsageTags.enableLocalDiskEncryption = false\n",
      "spark.databricks.tahoe.logStore.class = com.databricks.tahoe.store.DelegatingLogStore\n",
      "spark.hadoop.fs.s3.impl.disable.cache = true\n",
      "spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins = 30\n",
      "spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins = 30\n",
      "libraryDownload.sleepIntervalSeconds = 5\n",
      "spark.databricks.cloudProvider = AWS\n",
      "spark.sql.hive.convertMetastoreParquet = true\n",
      "spark.executor.id = driver\n",
      "spark.databricks.service.dbutils.server.backend = com.databricks.dbconnect.SparkServerDBUtils\n",
      "spark.databricks.repl.enableClassFileCleanup = true\n",
      "spark.databricks.clusterUsageTags.runtimeEngine = PHOTON\n",
      "spark.hadoop.fs.s3a.multipart.size = 10485760\n",
      "spark.databricks.clusterUsageTags.cloudProvider = AWS\n",
      "spark.metrics.conf = /databricks/spark/conf/metrics.properties\n",
      "spark.akka.frameSize = 256\n",
      "spark.hadoop.fs.s3a.fast.upload = true\n",
      "spark.sql.streaming.stopTimeout = 15s\n",
      "spark.hadoop.hive.server2.keystore.password = [REDACTED]\n",
      "spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting = false\n",
      "spark.databricks.unityCatalog.volumes.enabled = true\n",
      "spark.hadoop.fs.s3a.retry.interval = 250ms\n",
      "spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape = false\n",
      "spark.databricks.overrideDefaultCommitProtocol = org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\n",
      "spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass = com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient\n",
      "spark.databricks.clusterUsageTags.clusterNoDriverDaemon = false\n",
      "spark.hadoop.fs.adl.impl = com.databricks.common.filesystem.LokiFileSystem\n",
      "libraryDownload.timeoutSeconds = 180\n",
      "spark.hadoop.parquet.memory.pool.ratio = 0.5\n",
      "spark.databricks.passthrough.adls.gen2.tokenProviderClassName = com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider\n",
      "spark.databricks.unityCatalog.legacy.enableCrossScopeCredCache = true\n",
      "spark.hadoop.fs.s3a.block.size = 67108864\n",
      "spark.databricks.sparkContextId = 7833848033632012389\n",
      "spark.databricks.tahoe.logStore.gcp.class = com.databricks.tahoe.store.GCPLogStore\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.databricks.clusterUsageTags.sparkMasterUrlType = None\n",
      "spark.databricks.clusterUsageTags.sparkVersion = 13.3.x-photon-scala2.12\n",
      "spark.sql.sources.commitProtocolClass = com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol\n",
      "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs = 0\n",
      "spark.hadoop.fs.fcfs-s3a.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
      "spark.databricks.clusterUsageTags.attribute_tag_budget = \n",
      "spark.databricks.clusterUsageTags.clusterMaxWorkers = 8\n",
      "spark.databricks.clusterUsageTags.clusterWorkers = 2\n",
      "spark.databricks.clusterUsageTags.clusterPythonVersion = 3\n",
      "spark.databricks.clusterUsageTags.enableDfAcls = false\n",
      "spark.databricks.unityCatalog.queryFederation.enabled = true\n",
      "spark.databricks.cloudfetch.requestDownloadUrlsWithHeaders = true\n",
      "spark.hadoop.databricks.loki.fileSystemCache.enabled = true\n",
      "spark.shuffle.service.enabled = true\n",
      "spark.hadoop.fs.file.impl = com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem\n",
      "spark.plugins = org.apache.spark.sql.connect.SparkConnectPlugin\n",
      "spark.hadoop.fs.fcfs-wasb.impl.disable.cache = true\n",
      "spark.hadoop.fs.cpfs-s3.impl = com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
      "spark.databricks.clusterUsageTags.containerZoneId = auto\n",
      "spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer = \n",
      "spark.hadoop.fs.s3a.multipart.threshold = 104857600\n",
      "spark.repl.class.outputDir = /local_disk0/tmp/repl/spark-7833848033632012389-41c3de6e-d919-4a94-b38e-e8cceaa7db09\n",
      "spark.rpc.message.maxSize = 256\n",
      "spark.databricks.clusterUsageTags.attribute_tag_dust_suite = \n",
      "spark.hadoop.fs.fcfs-wasbs.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
      "spark.databricks.driverNfs.enabled = true\n",
      "spark.databricks.clusterUsageTags.clusterMetastoreAccessType = RDS_DIRECT\n",
      "spark.hadoop.parquet.page.metadata.validation.enabled = true\n",
      "spark.databricks.clusterUsageTags.instanceProfileUsed = false\n",
      "spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName = com.databricks.unity.TokenServiceApiTokenProvider\n",
      "spark.databricks.passthrough.glue.executorServiceFactoryClassName = com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory\n",
      "spark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus = false\n",
      "spark.databricks.acl.scim.client = com.databricks.spark.sql.acl.client.DriverToWebappScimClient\n",
      "spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick = false\n",
      "spark.databricks.clusterUsageTags.isSingleUserCluster = true\n",
      "spark.hadoop.fs.adl.impl.disable.cache = true\n",
      "spark.hadoop.parquet.block.size.row.check.max = 10\n",
      "spark.hadoop.fs.s3a.connection.maximum = 200\n",
      "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2 = 0\n",
      "spark.hadoop.fs.s3a.fast.upload.active.blocks = 32\n",
      "spark.shuffle.reduceLocality.enabled = false\n",
      "spark.hadoop.spark.sql.sources.outputCommitterClass = com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter\n",
      "spark.hadoop.fs.fcfs-abfs.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
      "spark.hadoop.databricks.loki.fileStatusCache.enabled = true\n",
      "spark.hadoop.fs.fcfs-abfss.impl.disable.cache = true\n",
      "spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled = false\n",
      "spark.hadoop.spark.hadoop.aws.glue.cache.table.size = 1000\n",
      "spark.sql.parquet.compression.codec = snappy\n",
      "spark.hadoop.fs.stage.impl = com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem\n",
      "spark.databricks.credential.scope.fs.s3a.tokenProviderClassName = com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider\n",
      "spark.databricks.cloudfetch.hasRegionSupport = true\n",
      "spark.databricks.clusterUsageTags.ngrokNpipEnabled = true\n",
      "spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories = false\n",
      "spark.databricks.clusterUsageTags.clusterId = 0313-214602-12tyza6o\n",
      "spark.hadoop.spark.hadoop.aws.glue.cache.db.size = 1000\n",
      "spark.databricks.clusterUsageTags.instanceWorkerEnvId = workerenv-1803637943354536-32fe5019-6b3c-4144-98f9-df63a5881b89\n",
      "spark.databricks.passthrough.glue.credentialsProviderFactoryClassName = com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory\n",
      "spark.databricks.clusterUsageTags.clusterOwnerOrgId = 1803637943354536\n",
      "spark.sparklyr-backend.threads = 1\n",
      "spark.hadoop.fs.fcfs-wasb.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\n",
      "spark.databricks.passthrough.s3a.tokenProviderClassName = com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider\n",
      "spark.databricks.session.share = false\n",
      "spark.databricks.clusterUsageTags.clusterUnityCatalogMode = SINGLE_USER\n",
      "spark.databricks.clusterUsageTags.clusterResourceClass = default\n",
      "spark.databricks.isShieldWorkspace = false\n",
      "spark.databricks.sql.initial.catalog.name = data_demo\n",
      "spark.hadoop.fs.idbfs.impl = com.databricks.io.idbfs.IdbfsFileSystem\n",
      "spark.hadoop.fs.dbfs.impl = com.databricks.backend.daemon.data.client.DbfsHadoop3\n",
      "spark.databricks.clusterUsageTags.clusterSku = STANDARD_SKU\n",
      "spark.app.id = app-20240314212135-0000\n",
      "spark.hadoop.fs.gs.impl.disable.cache = true\n",
      "spark.databricks.privateLinkEnabled = false\n",
      "spark.delta.sharing.profile.provider.class = io.delta.sharing.DeltaSharingCredentialsProvider\n",
      "spark.worker.aioaLazyConfig.iamReadinessCheckClientClass = com.databricks.backend.daemon.driver.NephosIamRoleCheckClient\n",
      "spark.databricks.clusterUsageTags.sparkImageLabel = release__13.3.x-snapshot-photon-scala2.12__databricks-universe__13.3.12__ca78acd__818fc34__jenkins__8ed39b5__format-3\n",
      "spark.databricks.clusterUsageTags.clusterEbsVolumeType = GENERAL_PURPOSE_SSD\n",
      "spark.hadoop.fs.wasbs.impl = com.databricks.common.filesystem.LokiFileSystem\n",
      "spark.databricks.clusterUsageTags.clusterScalingType = autoscaling\n",
      "spark.databricks.clusterUsageTags.clusterEbsVolumeCount = 3\n",
      "spark.databricks.clusterUsageTags.clusterAllTags = [{\"key\":\"Vendor\",\"value\":\"Databricks\"},{\"key\":\"Creator\",\"value\":\"hakim.mohasibi@gmail.com\"},{\"key\":\"ClusterName\",\"value\":\"Nacir Nacir's Cluster\"},{\"key\":\"ClusterId\",\"value\":\"0313-214602-12tyza6o\"},{\"key\":\"Name\",\"value\":\"workerenv-1803637943354536-32fe5019-6b3c-4144-98f9-df63a5881b89-worker\"}]\n",
      "spark.databricks.automl.serviceEnabled = true\n",
      "spark.hadoop.parquet.page.size.check.estimate = false\n",
      "spark.databricks.clusterUsageTags.attribute_tag_service = \n",
      "spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class = com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.databricks.metrics.filesystem_io_metrics = true\n",
      "spark.hadoop.spark.driverproxy.customHeadersToProperties = X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name,X-Databricks-Oauth-Identity-Custom-Claim:spark.databricks.oauthCustomIdentityClaims\n",
      "spark.databricks.unityCatalog.credentialScope.enabled = true\n",
      "spark.repl.class.uri = spark://10.233.171.33:34685/classes\n",
      "spark.databricks.cloudfetch.requesterClassName = com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester\n",
      "spark.databricks.delta.logStore.crossCloud.fatal = true\n",
      "spark.databricks.driverNfs.clusterWidePythonLibsEnabled = true\n",
      "spark.databricks.unityCatalog.enabled = true\n",
      "spark.files.fetchFailure.unRegisterOutputOnHost = true\n",
      "spark.databricks.clusterUsageTags.enableSqlAclsOnly = false\n",
      "spark.databricks.clusterUsageTags.clusterNumSshKeys = 0\n",
      "spark.databricks.clusterUsageTags.clusterSizeType = VM_CONTAINER\n",
      "spark.hadoop.databricks.fs.perfMetrics.enable = true\n",
      "spark.hadoop.fs.gs.outputstream.upload.chunk.size = 16777216\n",
      "spark.memory.offHeap.size = 1978662912\n",
      "spark.databricks.clusterUsageTags.currentAttemptContainerZoneId = eu-west-3c\n",
      "spark.speculation.quantile = 0.9\n",
      "spark.databricks.clusterUsageTags.privateLinkEnabled = false\n",
      "spark.shuffle.manager = SORT\n",
      "spark.files.overwrite = true\n",
      "spark.databricks.credential.aws.secretKey.redactor = com.databricks.spark.util.AWSSecretKeyRedactorProxy\n",
      "spark.databricks.clusterUsageTags.clusterNumCustomTags = 0\n",
      "spark.connect.extensions.command.classes = io.delta.connect.DeltaCommandPlugin\n",
      "spark.hadoop.fs.s3.impl = com.databricks.common.filesystem.LokiFileSystem\n",
      "spark.hadoop.fs.s3a.impl.disable.cache = true\n",
      "spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes = false\n",
      "spark.r.numRBackendThreads = 1\n",
      "spark.hadoop.fs.wasbs.impl.disable.cache = true\n",
      "spark.hadoop.fs.abfss.impl.disable.cache = true\n",
      "spark.hadoop.fs.azure.cache.invalidator.type = com.databricks.encryption.utils.CacheInvalidatorImpl\n",
      "spark.databricks.clusterUsageTags.clusterEbsVolumeSize = 100\n",
      "spark.sql.hive.metastore.version = 0.13.0\n",
      "spark.shuffle.service.port = 4048\n",
      "spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType = default\n",
      "spark.databricks.acl.client = com.databricks.spark.sql.acl.client.SparkSqlAclClient\n",
      "spark.streaming.driver.writeAheadLog.closeFileAfterWrite = true\n",
      "spark.hadoop.hive.warehouse.subdir.inherit.perms = false\n",
      "spark.databricks.clusterUsageTags.isServicePrincipalCluster = false\n",
      "spark.databricks.credential.scope.fs.impl = com.databricks.sql.acl.fs.CredentialScopeFileSystem\n",
      "spark.databricks.enablePublicDbfsFuse = false\n",
      "spark.databricks.clusterUsageTags.enableElasticDisk = false\n",
      "spark.hadoop.fs.fcfs-wasbs.impl.disable.cache = true\n",
      "spark.driver.host = 10.233.171.33\n",
      "spark.driver.port = 34685\n",
      "spark.databricks.passthrough.adls.tokenProviderClassName = com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider\n",
      "spark.app.name = Databricks Shell\n",
      "spark.driver.allowMultipleContexts = false\n",
      "spark.hadoop.fs.AbstractFileSystem.gs.impl = shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\n",
      "spark.databricks.secret.sparkConf.keys.toRedact = \n",
      "spark.rdd.compress = true\n",
      "spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException = false\n",
      "spark.hadoop.fs.s3a.retry.limit = 6\n",
      "spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env = \n",
      "spark.databricks.clusterUsageTags.isIMv2Enabled = true\n",
      "spark.databricks.eventLog.dir = eventlogs\n",
      "spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled = false\n",
      "spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName = com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider\n",
      "spark.databricks.driverNfs.pathSuffix = .ephemeral_nfs\n",
      "spark.databricks.clusterUsageTags.clusterCreator = Webapp\n",
      "spark.databricks.clusterUsageTags.workerEnvironmentId = workerenv-1803637943354536-32fe5019-6b3c-4144-98f9-df63a5881b89\n",
      "spark.speculation = false\n",
      "spark.hadoop.hive.server2.session.check.interval = 60000\n",
      "spark.sql.hive.convertCTAS = true\n",
      "spark.connect.extensions.relation.classes = io.delta.connect.DeltaRelationPlugin\n",
      "spark.hadoop.spark.sql.parquet.output.committer.class = org.apache.spark.sql.parquet.DirectParquetOutputCommitter\n",
      "spark.hadoop.fs.s3a.max.total.tasks = 1000\n",
      "spark.databricks.tahoe.logStore.aws.class = com.databricks.tahoe.store.MultiClusterLogStore\n",
      "spark.hadoop.fs.s3a.fast.upload.default = true\n",
      "spark.hadoop.fs.mlflowdbfs.impl = com.databricks.mlflowdbfs.MlflowdbfsFileSystem\n",
      "spark.databricks.eventLog.listenerClassName = com.databricks.backend.daemon.driver.DBCEventLoggingListener\n",
      "spark.hadoop.fs.abfs.impl.disable.cache = true\n",
      "spark.app.startTime = 1710451284411\n",
      "spark.speculation.multiplier = 3\n",
      "spark.storage.blockManagerTimeoutIntervalMs = 300000\n",
      "spark.databricks.clusterUsageTags.driverNodeType = m6i.large\n",
      "spark.databricks.clusterUsageTags.driverContainerPrivateIp = 10.233.171.33\n",
      "spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount = 3\n",
      "spark.memory.offHeap.enabled = true\n",
      "spark.sparkr.use.daemon = false\n",
      "spark.scheduler.listenerbus.eventqueue.capacity = 20000\n",
      "spark.hadoop.fs.s3a.impl = com.databricks.common.filesystem.LokiFileSystem\n",
      "spark.databricks.clusterUsageTags.clusterStateMessage = Starting Spark\n",
      "spark.hadoop.parquet.page.write-checksum.enabled = true\n",
      "spark.hadoop.databricks.s3commit.client.sslTrustAll = false\n",
      "spark.hadoop.fs.s3a.threads.max = 136\n",
      "spark.r.backendConnectionTimeout = 604800\n",
      "spark.ui.prometheus.enabled = true\n",
      "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs = 0\n",
      "spark.hadoop.fs.s3n.impl = com.databricks.common.filesystem.LokiFileSystem\n",
      "spark.hadoop.hive.server2.idle.session.timeout = 900000\n",
      "spark.databricks.redactor = com.databricks.spark.util.DatabricksSparkLogRedactorProxy\n",
      "spark.databricks.clusterUsageTags.autoTerminationMinutes = 120\n",
      "spark.executor.extraClassPath = /databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*\n",
      "spark.databricks.autotune.maintenance.client.classname = com.databricks.maintenanceautocompute.MACClientImpl\n",
      "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes = 0\n",
      "spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace = 0\n",
      "spark.hadoop.fs.fcfs-abfs.impl.disable.cache = true\n",
      "spark.databricks.clusterUsageTags.clusterLastActivityTime = 1710438658714\n",
      "spark.databricks.clusterUsageTags.instanceBootstrapType = vm-selfbootstrap\n",
      "spark.hadoop.parquet.page.verify-checksum.enabled = true\n",
      "spark.logConf = true\n",
      "spark.databricks.clusterUsageTags.enableJobsAutostart = true\n",
      "spark.databricks.clusterUsageTags.clusterNodeType = m6i.large\n",
      "spark.hadoop.hive.server2.enable.doAs = false\n",
      "spark.hadoop.parquet.filter.columnindex.enabled = false\n",
      "spark.shuffle.memoryFraction = 0.2\n",
      "spark.databricks.unityCatalog.volumes.fuse.server.enabled = true\n",
      "spark.hadoop.fs.dbfsartifacts.impl = com.databricks.backend.daemon.data.client.DBFSV1\n",
      "spark.hadoop.fs.cpfs-s3a.impl = com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\n",
      "spark.databricks.workerNodeTypeId = m6i.large\n",
      "spark.hadoop.fs.s3a.connection.timeout = 50000\n",
      "spark.databricks.secret.envVar.keys.toRedact = \n",
      "spark.databricks.clusterUsageTags.orgId = 1803637943354536\n",
      "spark.databricks.clusterUsageTags.clusterSpotBidPricePercent = 100\n",
      "spark.databricks.clusterUsageTags.clusterOwnerUserId = 6774191665194252\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Current Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# print all session Spark configurations \n",
    "for key, value in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{key} = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd160768-d5e2-490e-b251-5e0e37a0365c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##  New directory named datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae4a7f13-6ae3-4eb6-aaca-9c9e689c83b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"/datalake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc38850c-ff18-481c-8dd8-5dd058b7cb4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Three directories in it under datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664205ea-d584-4684-887e-ca0ffec7aaff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"/datalake/raw\")\n",
    "dbutils.fs.mkdirs(\"/datalake/curated\")\n",
    "dbutils.fs.mkdirs(\"/datalake/serving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5402be-ba4a-4011-b1db-40fdd94ac772",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/datalake/curated/</td><td>curated/</td><td>0</td><td>1710457041324</td></tr><tr><td>dbfs:/datalake/raw/</td><td>raw/</td><td>0</td><td>1710457041324</td></tr><tr><td>dbfs:/datalake/serving/</td><td>serving/</td><td>0</td><td>1710457041324</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/datalake/curated/",
         "curated/",
         0,
         1710457041324
        ],
        [
         "dbfs:/datalake/raw/",
         "raw/",
         0,
         1710457041324
        ],
        [
         "dbfs:/datalake/serving/",
         "serving/",
         0,
         1710457041324
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"/datalake\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_milestone_data_ingestion",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
